{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pavti7/DATA_Pawel_A/blob/main/Machine_Learning/Klasyfikacja/Podsumowanie.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcdf7b5a",
      "metadata": {
        "id": "dcdf7b5a"
      },
      "source": [
        "### Regresja logistyczna:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7adcd3c0",
      "metadata": {
        "id": "7adcd3c0"
      },
      "source": [
        "#### Idea: \n",
        "\n",
        "Wyznaczenie parametrów funkcji metodą spadku gradientu. Rozwiązywanie układu równań z użyciem funckji sigmoidalnej."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6e18649",
      "metadata": {
        "id": "b6e18649"
      },
      "source": [
        "#### Implementacja:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "269a4e2e",
      "metadata": {
        "id": "269a4e2e"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "rl = LogisticRegression()\n",
        "rl.fit(X_train, y_train)\n",
        "\n",
        "y_pred = rl.predict(X_test)\n",
        "\n",
        "# Lub jeśli chcemy otrzymać prawdopodobieństwa:\n",
        "y_pred = rl.predict_proba(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efcd3394",
      "metadata": {
        "id": "efcd3394"
      },
      "source": [
        "#### Metody regularyzacji: \n",
        "\n",
        "Zapobiega przetrenowaniu. Dodaje parametr w funkcji kosztu, ograniczający wielkość wag:\n",
        "\n",
        "* regresja lasso - L1, człon regularyzacji to suma wartości bezwzględnych wszystkich wag(Reg=||w||). Ponieważ rozważana jest wartość bezwzględna wag, metoda regularyzacji L1 może prowadzić do zmniejszenia ich do zera, dzięki czemu możliwa jest selekcja cech (poprzez przypisanie nieistotnym cechom wejściowym wag zerowych) oraz kompresja modelu, często jednak może ona prowadzić do zjawiska underfittingu.\n",
        "\n",
        "* regresja grzbietowa - L2, człon ten ma postać Reg=||w||^2, co powoduje zmniejszanie wartości wag, lecz nie do zera (uzasadnienie). Metoda ta nie przeprowadza zatem selekcji cech i jest preferowana w większości sytuacji, gdyż prowadzi do uwzględnienia wszystkich danych wejściowych."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1dc8b07",
      "metadata": {
        "id": "f1dc8b07"
      },
      "outputs": [],
      "source": [
        "## Regresja lasso:\n",
        "rlr = LogisticRegression(C=0.01, solver = 'liblinear', penalty='l1')\n",
        "rlr.fit(X_train, y_train)\n",
        "y_pred = rlr.predict(X_test)\n",
        "\n",
        "## Regresja grzbietowa:\n",
        "rlr = LogisticRegression(random_state=30, solver = 'liblinear', penalty = 'l2', C = 0.5)\n",
        "rlr.fit(X_train, y_train)\n",
        "y_pred = rlr.predict(X_test)\n",
        "\n",
        "# Dla obu działa też funkcja predict_proba()."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "318662ec",
      "metadata": {
        "id": "318662ec"
      },
      "source": [
        "### KNN:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e5cc47a",
      "metadata": {
        "id": "1e5cc47a"
      },
      "source": [
        "#### Idea:\n",
        "Porównanie odległości próki ze zbioru testowego do próbek zbioru treningowego. Algorytm leniwy: przechowuje dane treningowe w pamięci. Liczy odległość dopiero w momencie pojawienia się próbki testowej. \n",
        "\n",
        "Najważniejsze parametry to:\n",
        "* metric - określa, wykorzystany typ odległości,\n",
        "* n_neigbours - określa liczbę sąsiadów, kla których liczone jest podobieństwo."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ab11013",
      "metadata": {
        "id": "8ab11013"
      },
      "source": [
        "#### Implementacja:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "007d8ee8",
      "metadata": {
        "id": "007d8ee8"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "y_pred_knn = knn.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d24bf55",
      "metadata": {
        "id": "4d24bf55"
      },
      "source": [
        "### SVM:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e94deb2e",
      "metadata": {
        "id": "e94deb2e"
      },
      "source": [
        "#### Idea:\n",
        "\n",
        "SVM maksymalizuje margines pomiędzy danymi opierając się na wektorach wsparcia. Klasyfikator SVM bierze pod uwagę tylko próbki położone blisko\n",
        "granicy klas - tak zwane wektory wsparcia i na ich podstawie rozdziela cały zbiór.\n",
        "\n",
        "Pozwala na modyfikację przestrzeni za pomocą jądra przekształcenia. Do przestrzeni dodwanay jest dodatkowy wymiar, co może spowodować, że nieseparowalne dane staną się separowalne.\n",
        "\n",
        "Najwazniejsze parametry:\n",
        "* kernel - określa transformację przestrzeni,\n",
        "* C - paramentr regularyzacji.\n",
        "\n",
        "Regularyzacja: L2.\n",
        "\n",
        "Nie zwraca prawdopodobieństw, że próbka należy do danej klasy, jedynie klasę."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44a0ffd4",
      "metadata": {
        "id": "44a0ffd4"
      },
      "source": [
        "#### Implementacja:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e81739a4",
      "metadata": {
        "id": "e81739a4"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "svm = SVC(kernel = 'linear', C =0.5, random_state=30)\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "y_pred = svm.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4d13bac",
      "metadata": {
        "id": "e4d13bac"
      },
      "source": [
        "### Klasyfikator Naiwny Bayesa:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfbb0702",
      "metadata": {
        "id": "cfbb0702"
      },
      "source": [
        "#### Idea:\n",
        "\n",
        "Obliczenie prawdopodobieństwa, że próbka należy do danej klasy w oparciu o twierdzenie Bayesa. Zakłada niezależność cech."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89cd5d77",
      "metadata": {
        "id": "89cd5d77"
      },
      "source": [
        "#### Implementacja:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64200cf8",
      "metadata": {
        "id": "64200cf8"
      },
      "outputs": [],
      "source": [
        "# MultinomialNB stosowany jeśli cechy mają rozkład dyskretny:\n",
        "\n",
        "mnb = MultinomialNB()\n",
        "mnb.fit(X_train, y_train)\n",
        "\n",
        "y_pred = mnb.predict(X_test)\n",
        "\n",
        "# GaussianNB stosowany jeśli cechy mają rozkład dyskretny:\n",
        "\n",
        "gnb = GaussianNB() \n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "y_pred = gnb.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70745e3b",
      "metadata": {
        "id": "70745e3b"
      },
      "source": [
        "### Drzewa decyzyjne:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7a80c02",
      "metadata": {
        "id": "c7a80c02"
      },
      "source": [
        "#### Idea:\n",
        "\n",
        "Drzewo budujemy zaczynając od atrybutu, który niesie najwięcej informacji - umożliwia najlepszy podział na klasy. Jest to korzeń drzewa. Następnie przechodzimy do poziomów niżej, gdzie występują kolejne podziały (węzły), a jeśli już nie ma podziału wskazywana jest klasa wartości (liście). Wybór atrybutu i podziału wartości według kryterium maksymalnego\n",
        "zysku informacji.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "504b2753",
      "metadata": {
        "id": "504b2753"
      },
      "source": [
        "#### Przyklładowe metody regularyzacji:\n",
        "* ograniczenie głębokości drzewa,\n",
        "* ograniczenie wielkości liści,\n",
        "* przycinanie.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0583fd3a",
      "metadata": {
        "id": "0583fd3a"
      },
      "source": [
        "#### Implementacja:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01b12d2f",
      "metadata": {
        "id": "01b12d2f"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "dt = DecisionTreeClassifier()\n",
        "dt.fit(X_train, y_train)\n",
        "\n",
        "y_pred = dt.predict(X_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4970d61",
      "metadata": {
        "id": "f4970d61"
      },
      "source": [
        "# Zespoły klasyfikatorów"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb17f318",
      "metadata": {
        "id": "cb17f318"
      },
      "source": [
        "### Bagging"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46f51585",
      "metadata": {
        "id": "46f51585"
      },
      "source": [
        "BaggingClassifier - Dopasowuje klasyfikatory bazowe każdy na losowych podzbiorach oryginalnego zbioru danych, a następnie agreguje ich indywidualne przewidywania (poprzez głosowanie lub uśrednianie), aby utworzyć ostateczną predykcję. Taki metaestymator może być zazwyczaj użyty jako sposób na zmniejszenie wariancji estymatora typu black-box (np. drzewa decyzyjnego), poprzez wprowadzenie losowości do jego procedury konstrukcyjnej, a następnie stworzenie z niego zespołu.\n",
        "\n",
        "Innymi słowy: losujemy ze zwracaniem n próbek, na każdej wykonujemy klasyfikację, a następnie każdy klasyfikator \"głosuje\" na klasę, którą przewidział."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39ca43e7",
      "metadata": {
        "id": "39ca43e7"
      },
      "outputs": [],
      "source": [
        "#Losowanie ze zwracaniem z dataframe X:\n",
        "    \n",
        "import numpy as np\n",
        "\n",
        "N = len(y)\n",
        "inds = np.array(range(N))\n",
        "\n",
        "for i in range(1):\n",
        "    choosen = np.random.choice(inds, N)\n",
        "    X_boostrap = X.iloc[choosen]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e64ec8c3",
      "metadata": {
        "id": "e64ec8c3"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "\n",
        "clf = BaggingClassifier(base_estimator=DecisionTreeClassifier())\n",
        "\n",
        "clf.fit(train_data, train_labels)\n",
        "\n",
        "y_pred = clf.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6e9b20a",
      "metadata": {
        "id": "a6e9b20a"
      },
      "source": [
        "### Random Forrest"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c0189ca",
      "metadata": {
        "id": "4c0189ca"
      },
      "source": [
        "Dalsze rozwinięcie Baggingu.\n",
        "\n",
        "Trenowane jest wiele drzew decyzyjnych, a następnie następuje głosowanie:\n",
        "    \n",
        "* Random subspace method - wybór podzbioru cech dla każdego drzewa (budujemy kilka osobnych drzew, do każdego dajemy tylko określoną liczbę atrybutów - nie wszystkie)\n",
        "* Dodatkowo: Extremely Randomized Trees może wybierać losowy podzbiór cech w każdym węźle (przydatne gdy mamy do czynienia z overfittingiem)\n",
        "* w scikit-learn: oparte na na uśrednianiu predict_proba, a nie samej predykcji"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8749c92",
      "metadata": {
        "id": "c8749c92"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "clf = RandomForestClassifier()\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "y_pred_proba = clf.predict_proba(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7b0439e",
      "metadata": {
        "id": "f7b0439e"
      },
      "source": [
        "### Boosting"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1af8c01",
      "metadata": {
        "id": "e1af8c01"
      },
      "source": [
        "Szereg modeli budowanych iteracyjnie. \n",
        "\n",
        "podejście zachłanne:\n",
        "* pierwszy model trenowany jest na całym zbiorze\n",
        "* kolejne modele trenowane są na zmodyfikowanych zbiorach - źle\n",
        "zaklasyfikowane przykłady są rozpatrywane z większą wagą\n",
        "* Przykłady: Adaboost, LGBM, XGboost, Catboost.\n",
        "\n",
        "\n",
        "\n",
        "W przeciwieństwie do baggingu, tworzenie zbioru nie polega na losowości, ale zależy od rezultatów poprzednich modeli: każdy nowy podzbiór zawiera te przykłady, które były źle klasyfikowane przez poprzednie modele.\n",
        "\n",
        "Jak już dobrze poznacie koncepcję boostingu, opis algorytmów: https://towardsdatascience.com/catboost-vs-lightgbm-vs-xgboost-c80f40662924"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90a1945f",
      "metadata": {
        "id": "90a1945f"
      },
      "outputs": [],
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "xgb = xgb.XGBClassifier()\n",
        "\n",
        "xgb.fit(X_train, y_train)\n",
        "\n",
        "y_pred = xgb.predict(X_test)\n",
        "y_pred_proba = xgb.predict_proba(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4abb35d2",
      "metadata": {
        "id": "4abb35d2"
      },
      "source": [
        "# Inne ważne metody pomocnicze używane w data science:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "107ffa05",
      "metadata": {
        "id": "107ffa05"
      },
      "source": [
        "### Grid Search\n",
        "\n",
        "Metoda przeszukiwania podanych parametrów modelu, w celu znalezienia najlepszych dla danego zboiru danych."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10247011",
      "metadata": {
        "id": "10247011"
      },
      "outputs": [],
      "source": [
        "# Przykład na modelu SVM:\n",
        "\n",
        "svm = SVC()\n",
        "\n",
        "# Słownik przeszukiwanych parametrów i uruchomienie metody:\n",
        "parameters = {'C':[0.01, 0.1, 1, 10, 100],\n",
        "              'gamma':[0.0001, 0.001, 0.01, 0.1, 1, 10],\n",
        "             'kernel':['rbf', 'linear']}\n",
        "\n",
        "searcher = GridSearchCV(svm, parameters)\n",
        "searcher.fit(X_train, y_train)\n",
        "\n",
        "# Wyświelenie najlepszych parametrów i wyniku dla nich:\n",
        "print(\"Best CV params\", searcher.best_params_)\n",
        "print(\"Best CV accuracy\", searcher.best_score_)\n",
        "\n",
        "# Wyświetlenie dokładności na zbiorze testowym:\n",
        "print(\"Test accuracy of best grid search hypers:\", searcher.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fed50cba",
      "metadata": {
        "id": "fed50cba"
      },
      "source": [
        "### Walidacja krzyżowa:\n",
        "\n",
        "Metoda pomagająca wybrać optymalny dla danego zastosowania model.  K-krotna walidacja krzyżowa oznacza, że zbiór danych jest podzielony na K części. W każdej iteracji inna z części używana jest jako zbiór walidacyjny. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c84f424",
      "metadata": {
        "id": "3c84f424"
      },
      "source": [
        "Cały proces budowy i wykorzystania modelu możemy podzielić na trzy fazy:\n",
        "1. Proces wyboru najlepszego modelu, dobór zmiennych, dobór hiperparametrów modelu (np. głębokość trzewa, stopień regularyzacji). W tym procesie dzielimy sobie zbiór. I teraz: potrzebujemy zbioru na którym będziemy trenować i na którym będziemy oceniać jakość różnych wariantów modeli / zmiennych / hiperparametrów:\n",
        "\n",
        "      a. W najprostszym wariancie może to być zbiór treningowy i testowy. Na treningowym trenujemy model, na testowym testujemy i wyliczamy metryki jakości. Takie rozwiązanie ma jednak dwie zasadnicze wady:\n",
        "       \n",
        "           i. możemy nie dokonać wyboru faktycznie najlepszego modelu, tylko takiego, który najlepiej dopasował się do tego przypadkowo wylosowanego zbioru testowego.\n",
        "           ii. Możemy np. podczas doboru hiperparametrów dopasować optymalne hiperparametry do tego konkretnego zbioru testowego. Nie jest powiedziane, że dla wylosowanego inaczej będą takie same. W takim wypadku model dopasuje nam się za bardzo do zbioru testowego.\n",
        "           \n",
        "    b. W nieco bardziej skomplikowanym wariancie dzielimy zbiór na treningowy, walidacyjny i testowy. Pozbywamy się tu problemu a. ii. Porównując modele ze sobą i dobierając hiperparaetry używamy zbioru walidacyjnego, a na samym końcu procesu, na wszelki wypadek testujemy wybrane modele jeszcze raz na zbiorze testowym (nie używanym wcześniej nigdzie). Dzięki temu potwierdzamy po prostu, że model zadziała równie dobrze, na zbiorze danych, których jeszcze nigdy nie widział.\n",
        "    \n",
        "    c. W najciekawszym wariancie dzielimy zbiór na treningowy i testowy ale na treningowym wykonujemy walidację krzyżową. Walidacja krzyżowa polega na podziale zbioru na części (przyjmijmy:  f, g ,h, i, j). Każdą z wersji modelu / zmiennych / hiperparametrów, trenujemy wówczas 5 razy używając jako zbioru treningowego : walidacyjnego kolejno: f,g,h,i : j,    f,g,h,j : i,    f,g,i,j : h,    f,h,i,j : g,    g,h,i,j : f.\n",
        "    \n",
        "    Dzięki temu unikamy obu problemów, z punktów a. i. oraz a. i.i. Modele trenowane i oceniane są kilkukrotnie, na różnych zbiorach walidacyjnych. Wyniki miar są uśredniane z kilku prób (tu pozbywamy się problemu a. i.). A na koniec jeszcze dla pewności sprawdzamy czy nie dostosowaliśmy się za bardzo do wylosowanych części, testując na odłożonym wcześniej zbiorze testowym. To na którym zbiorze trenujemy, a na którym walidujemy czy testujemy (generalnie oceniamy) jakość jest więc trochę umowne.\n",
        "\n",
        "\n",
        "\n",
        "2. Jak mamy już wybrany najlepszy model / parametry / dobraliśmy dobrze zmienne do modelu, to trenujemy go na wszystkich dostępnych obserwacjach i taki model można wtedy wykorzystać do oceny już danych, których nie znamy.\n",
        "\n",
        "\n",
        "\n",
        "3. Wykorzystanie modelu – wybrany i wytrenowany model używamy do klasyfikacji danych, których nie znamy. To jest już faktyczne działanie tego modelu. np. wykrywanie nieprawidłowych transakcji, spamu, tego czy osoba jest chora itd."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8a945f4",
      "metadata": {
        "id": "f8a945f4"
      },
      "outputs": [],
      "source": [
        "# Przykład na modelu regresji logistycznej:\n",
        "\n",
        "from sklearn.model_selection import cross_validate\n",
        "\n",
        "lr = LogisticRegression(random_state=30)\n",
        "\n",
        "scores = cross_validate(lr, X_train, y_train, scoring = ['precision_macro', 'recall_macro'], cv = 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3d04834",
      "metadata": {
        "id": "f3d04834"
      },
      "source": [
        "### Metody kategoryzacji zmiennych ciągłych:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4511595",
      "metadata": {
        "id": "a4511595"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def equal_length(column, n_bins):\n",
        "    # metoda pandasa do podziału zbioru na jednakową frakwencję (częstość)\n",
        "    indices, bins = pd.cut(column, n_bins, labels=False, retbins=True, duplicates='drop')\n",
        "    return indices.map(dict(enumerate(bins)))\n",
        "\n",
        "def equal_frequency(column, n_bins):\n",
        "    # metoda pandasa do podziału zbioru na jednakową frakwencję (częstość)\n",
        "    indices, bins = pd.qcut(column, n_bins, labels=False, retbins=True, duplicates='drop') \n",
        "    return indices.map(dict(enumerate(bins)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "433b5d79",
      "metadata": {
        "id": "433b5d79"
      },
      "outputs": [],
      "source": [
        "length_binning = lambda x: equal_length(x, 5)\n",
        "freq_binning = lambda x: equal_frequency(x, 5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fae2a693",
      "metadata": {
        "id": "fae2a693"
      },
      "outputs": [],
      "source": [
        "X_l = X.apply(length_binning)\n",
        "X_f = X.apply(freq_binning)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b904763",
      "metadata": {
        "id": "1b904763"
      },
      "source": [
        "### Metody radzenia sobie ze zmiennymi kategorycznymi:"
      ]
    },
    {
      "cell_type": "raw",
      "id": "b5b7f0af",
      "metadata": {
        "id": "b5b7f0af"
      },
      "source": [
        "Metody zamieniające wartości kateorialne na macierz zer i jedynek:\n",
        "* pandas: get_dummies()\n",
        "* sklearn: OneHotEncoder()\n",
        "    \n",
        "Przykładowo kolumna:\n",
        "\n",
        "| Gatunek |\n",
        "|---------|\n",
        "| Pies    |\n",
        "| Pies    |\n",
        "| Kot     |\n",
        "| Kot     |\n",
        "| Delfin  |\n",
        "\n",
        "Zostanie zamieniona na:\n",
        "\n",
        "| Pies | Kot | Delfin |\n",
        "|------|-----|--------|\n",
        "| 1    | 0   | 0      |\n",
        "| 1    | 0   | 0      |\n",
        "| 0    | 1   | 0      |\n",
        "| 0    | 1   | 0      |\n",
        "| 0    | 0   | 1      |\n",
        "\n",
        "Umożliwi to wykorzystanie zmiennej przez model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd2740b1",
      "metadata": {
        "id": "fd2740b1"
      },
      "outputs": [],
      "source": [
        "# get_dummies()\n",
        "\n",
        "# Otrzymane w ten sposób kolumny należy dołączyć funkcją concat do zboiru X.\n",
        "dumies_columns = pd.get_dummies(data, columns = [])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b37dd06c",
      "metadata": {
        "id": "b37dd06c"
      },
      "source": [
        "### Scalery\n",
        "\n",
        "Skalowanie jest wykonywane podczas wstępnego przetwarzania danych, aby obsłużyć zróżnicowane wielkości lub jednostki (np. model może uznać za ważniejszą zmienną, która z natury przyjmuje większe wartości, nie dlatego, że zmienna ta faktycznie jest ważeniejsza, a dlatego, że model widzi ją jako większą wartość).\n",
        "\n",
        "\n",
        "Algorytmy uczenia maszynowego, które nie wymagają skalowania cech, to przede wszystkim nieliniowe algorytmy ML, takie jak drzewa decyzyjne, Random Forest, AdaBoost, Naive Bayes, itd."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cff16799",
      "metadata": {
        "id": "cff16799"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "minmaxscaler = MinMaxScaler()\n",
        "minmaxscaler.fit(X)\n",
        "\n",
        "X_scaled= minmaxscaler.transform(X.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f81a4352",
      "metadata": {
        "id": "f81a4352"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "standardizer = StandardScaler()\n",
        "standardizer.fit(X)\n",
        "\n",
        "X_normalized = normalizer.transform(X.values)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afd00482",
      "metadata": {
        "id": "afd00482"
      },
      "source": [
        "### Pipeliny:\n",
        "\n",
        "Tworzenie sekwencji przetwarzania danych. Zamykanie modyfikacji danych (dummy, scalery, dyskretyzacja) i modelu w jednej funkcji, dzięki czemu możemy wykonać ją w walidacji krzyżowej dla wielu różnych transformatorów i modeli w pętli.\n",
        "\n",
        "Pośrednie kroki potoku muszą być \"transformatorami\", to znaczy muszą implementować metody fit i transform. Końcowy estymator musi tylko zaimplementować fit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b0e30f0",
      "metadata": {
        "id": "5b0e30f0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "621649e4",
      "metadata": {
        "id": "621649e4"
      },
      "outputs": [],
      "source": [
        "def load_dataset(filename, class_column, index_col=None):\n",
        "    dataset = pd.read_csv(f'ML-datasets/{filename}.csv', index_col=index_col)\n",
        "    dataset['class'] = dataset[class_column].astype('category').cat.codes\n",
        "    classes = dataset.pop(class_column).unique()\n",
        "    return dataset, classes\n",
        "\n",
        "def prepare_dataset(dataset_name):\n",
        "    params = {'iris': {'class_column': 'species', 'index_col': None},\n",
        "              'wine': {'class_column': 'Class', 'index_col': None},\n",
        "              'glass': {'class_column': 'Type', 'index_col': 'ID'}}\n",
        "    dataset, classes = load_dataset(dataset_name,\n",
        "                                    **params[dataset_name])\n",
        "    y = dataset.pop('class')\n",
        "    X = dataset\n",
        "    return X, y, classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b270d43e",
      "metadata": {
        "id": "b270d43e"
      },
      "outputs": [],
      "source": [
        "X, y, classes = prepare_dataset('wine')\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60cd4850",
      "metadata": {
        "id": "60cd4850",
        "outputId": "501a72e0-3a7e-4712-918d-c95ea422b576"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Alcohol</th>\n",
              "      <th>Malic acid</th>\n",
              "      <th>Ash</th>\n",
              "      <th>Alcalinity of ash</th>\n",
              "      <th>Magnesium</th>\n",
              "      <th>Total phenols</th>\n",
              "      <th>Flavanoids</th>\n",
              "      <th>Nonflavanoid phenols</th>\n",
              "      <th>Proanthocyanins</th>\n",
              "      <th>Color intensity</th>\n",
              "      <th>Hue</th>\n",
              "      <th>OD280/OD315 of diluted wines</th>\n",
              "      <th>Proline</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>167</th>\n",
              "      <td>12.82</td>\n",
              "      <td>3.37</td>\n",
              "      <td>2.30</td>\n",
              "      <td>19.5</td>\n",
              "      <td>88</td>\n",
              "      <td>1.48</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.97</td>\n",
              "      <td>10.26</td>\n",
              "      <td>0.72</td>\n",
              "      <td>1.75</td>\n",
              "      <td>685</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>14.30</td>\n",
              "      <td>1.92</td>\n",
              "      <td>2.72</td>\n",
              "      <td>20.0</td>\n",
              "      <td>120</td>\n",
              "      <td>2.80</td>\n",
              "      <td>3.14</td>\n",
              "      <td>0.33</td>\n",
              "      <td>1.97</td>\n",
              "      <td>6.20</td>\n",
              "      <td>1.07</td>\n",
              "      <td>2.65</td>\n",
              "      <td>1280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>13.11</td>\n",
              "      <td>1.01</td>\n",
              "      <td>1.70</td>\n",
              "      <td>15.0</td>\n",
              "      <td>78</td>\n",
              "      <td>2.98</td>\n",
              "      <td>3.18</td>\n",
              "      <td>0.26</td>\n",
              "      <td>2.28</td>\n",
              "      <td>5.30</td>\n",
              "      <td>1.12</td>\n",
              "      <td>3.18</td>\n",
              "      <td>502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>12.70</td>\n",
              "      <td>3.87</td>\n",
              "      <td>2.40</td>\n",
              "      <td>23.0</td>\n",
              "      <td>101</td>\n",
              "      <td>2.83</td>\n",
              "      <td>2.55</td>\n",
              "      <td>0.43</td>\n",
              "      <td>1.95</td>\n",
              "      <td>2.57</td>\n",
              "      <td>1.19</td>\n",
              "      <td>3.13</td>\n",
              "      <td>463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>14.83</td>\n",
              "      <td>1.64</td>\n",
              "      <td>2.17</td>\n",
              "      <td>14.0</td>\n",
              "      <td>97</td>\n",
              "      <td>2.80</td>\n",
              "      <td>2.98</td>\n",
              "      <td>0.29</td>\n",
              "      <td>1.98</td>\n",
              "      <td>5.20</td>\n",
              "      <td>1.08</td>\n",
              "      <td>2.85</td>\n",
              "      <td>1045</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Alcohol  Malic acid   Ash  Alcalinity of ash  Magnesium  Total phenols  \\\n",
              "167    12.82        3.37  2.30               19.5         88           1.48   \n",
              "16     14.30        1.92  2.72               20.0        120           2.80   \n",
              "66     13.11        1.01  1.70               15.0         78           2.98   \n",
              "79     12.70        3.87  2.40               23.0        101           2.83   \n",
              "8      14.83        1.64  2.17               14.0         97           2.80   \n",
              "\n",
              "     Flavanoids  Nonflavanoid phenols  Proanthocyanins  Color intensity   Hue  \\\n",
              "167        0.66                  0.40             0.97            10.26  0.72   \n",
              "16         3.14                  0.33             1.97             6.20  1.07   \n",
              "66         3.18                  0.26             2.28             5.30  1.12   \n",
              "79         2.55                  0.43             1.95             2.57  1.19   \n",
              "8          2.98                  0.29             1.98             5.20  1.08   \n",
              "\n",
              "     OD280/OD315 of diluted wines  Proline  \n",
              "167                          1.75      685  \n",
              "16                           2.65     1280  \n",
              "66                           3.18      502  \n",
              "79                           3.13      463  \n",
              "8                            2.85     1045  "
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28aceb09",
      "metadata": {
        "id": "28aceb09"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41ca2337",
      "metadata": {
        "id": "41ca2337"
      },
      "outputs": [],
      "source": [
        "transformer = Pipeline(steps=[('scaler', MinMaxScaler())]) \n",
        "\n",
        "# Można tu wrzucić kategoryzację, imputację, czyszczenie danych itd.\n",
        "# Można również wybrać dla różnych kolumn zrobić różne operacje.\n",
        "\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=[('data_transformers', transformer, ['Alcohol', 'Malic acid'])]) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d2a3860",
      "metadata": {
        "id": "0d2a3860"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "\n",
        "pipeline = Pipeline(steps = [\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('model',RandomForestClassifier())])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10fb18a0",
      "metadata": {
        "id": "10fb18a0"
      },
      "outputs": [],
      "source": [
        "rf_classifier = pipeline.fit(X_train, y_train)\n",
        "\n",
        "y_pred = rf_classifier.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cdadcce",
      "metadata": {
        "id": "4cdadcce",
        "outputId": "77824544-370b-4a37-be44-ce54be4a51c3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[14,  0,  1],\n",
              "       [ 0,  8,  4],\n",
              "       [ 0,  1, 17]], dtype=int64)"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "confusion_matrix(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8fef526",
      "metadata": {
        "id": "f8fef526"
      },
      "source": [
        "### Co (jeszcze) warto umieć / wiedzieć:\n",
        "* podstawy bash,\n",
        "* github,\n",
        "* zarządzanie swoim środowiskiem witrualnym (np. po to, żeby lokalnie posiadać różne wersje pythona), np. https://www.youtube.com/watch?v=mIB7IZFCE_k,\n",
        "* obsługa plików tekstowych (wyrażenia regularne: https://www.w3schools.com/python/python_regex.asp), czytanie i parsowanie plików linia po linii (przykład poniżej) - przydatne dla zaśmieconych danych dla niestandardowych formatów (np. wczytanie pdf).\n",
        "* praca na czystym pythonie, i numpy (słowniki, listy, zbiory, macierze),\n",
        "* budowanie modeli, raportów w formie bardziej przystępnej do wdrożenia produkcyjnego (skrypty .py, programowanie obiektowe, pipeliny),\n",
        "* metodyka pracy w IT: Agile, Kanban (uważam, że Kanban jest super do Data Science),\n",
        "* praca na większych danych, które nie mieszczą się w pamięci (przetestowanie np. chunków w pandasie, iterowanie pliku linia po linii i czysczenie)\n",
        "\n",
        "Polecane przeze mnie książki: https://helion.pl/ksiazki/uczenie-maszynowe-z-uzyciem-scikit-learn-i-tensorflow-wydanie-ii-aur-lien-g-ron,uczem2.htm#format/d"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "424ff168",
      "metadata": {
        "id": "424ff168"
      },
      "source": [
        "#### Przykładowe iterowanie po liniach pliku tekstowego w pythonie:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42cbf759",
      "metadata": {
        "id": "42cbf759"
      },
      "source": [
        "w funkcji open() podajemy parametr mode. Określa on w jaki sposób chcemy użyć printowanych linii. Można również zapisywać do pliku printowane linie. Tutaj opis: http://www.manpagez.com/man/3/fopen/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fae6bf05",
      "metadata": {
        "id": "fae6bf05",
        "outputId": "01a902c6-dda5-4414-94a5-79eeb7d3acd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Date,Open,High,Low,Close,Volume,Adj Close\n",
            "\n",
            "2014-07-08,96.27,96.80,93.92,95.35,65130000,95.35\n",
            "\n",
            "2014-07-07,94.14,95.99,94.10,95.97,56305400,95.97\n",
            "\n",
            "2014-07-03,93.67,94.10,93.20,94.03,22891800,94.03\n",
            "\n",
            "2014-07-02,93.87,94.06,93.09,93.48,28420900,93.48\n",
            "\n",
            "2014-07-01,93.52,94.07,93.13,93.52,38170200,93.52\n",
            "\n",
            "2014-06-30,92.10,93.73,92.09,92.93,49482300,92.93\n",
            "\n",
            "2014-06-27,90.82,92.00,90.77,91.98,64006800,91.98\n",
            "\n",
            "2014-06-26,90.37,91.05,89.80,90.90,32595800,90.90\n",
            "\n",
            "2014-06-25,90.21,90.70,89.65,90.36,36852200,90.36\n",
            "\n"
          ]
        }
      ],
      "source": [
        "N = 10\n",
        "with open('./ML-datasets/apple.csv', 'r') as f:\n",
        "    counter = 0\n",
        "    for line in f:\n",
        "        print(line)\n",
        "        counter += 1\n",
        "        if counter == N:\n",
        "            break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43cdbf76",
      "metadata": {
        "id": "43cdbf76"
      },
      "source": [
        "# Inne:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "265bfcc2",
      "metadata": {
        "id": "265bfcc2"
      },
      "source": [
        "### Przydatne rozwiązania i strony:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9201c63",
      "metadata": {
        "id": "e9201c63"
      },
      "source": [
        "#### MLFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8823f4a",
      "metadata": {
        "id": "f8823f4a"
      },
      "source": [
        "MLflow to platforma open source do zarządzania Machine Learningiem, w tym eksperymentami, odtwarzalnością, wdrożeniem i centralnym rejestrem modeli.\n",
        "\n",
        "https://mlflow.org/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00ebffd2",
      "metadata": {
        "id": "00ebffd2"
      },
      "source": [
        "Jak zainstalować i zastosować:\n",
        "\n",
        "https://towardsdatascience.com/using-mlflow-to-track-machine-learning-experiments-adbf27e9d36c"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4f41d74",
      "metadata": {
        "id": "b4f41d74"
      },
      "source": [
        "#### Kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bf0d757",
      "metadata": {
        "id": "0bf0d757"
      },
      "source": [
        "Strona ze zbiorami danych do trenigu, konkursami, a także kursami: https://www.kaggle.com/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff2f3511",
      "metadata": {
        "id": "ff2f3511"
      },
      "source": [
        "#### Dodatki do Jupyter Notebooka:\n",
        "\n",
        "* biblioteka: jupyter_contrib_nbextensions - zarządzanie notebookiem (np. spis treści, łatwe poruszanie się między nagłówkami)\n",
        "* biblioteka: jupyterthemes - tło w notebooku, \n",
        "* IPython Magic Commands (np. %%time na początku komórki, pokaże czas wykonywiania)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8dc92869",
      "metadata": {
        "id": "8dc92869"
      },
      "source": [
        "### Algorytmy, z którymi spotkałem się w pracy / w literaturze:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10138f33",
      "metadata": {
        "id": "10138f33"
      },
      "source": [
        "#### Szeregi czasowe\n",
        "\n",
        "Szereg czasowy to ciąg obserwacji pokazujący kształtowanie się badanego zjawiska w kolejnych okresach (dniach, miesiącach, kwartałach, latach, itp.). W szeregu czasowym można wyodrębnić kilka składowych będących wynikiem wpływu różnych czynników na dane zjawisko.\n",
        "\n",
        "Wyróżnia się następujące składowe szeregów czasowych:\n",
        "\n",
        "* Tendencja rozwojowa, zwana trendem, wyraża długookresową skłonność do jednokierunkowych zmian (wzrostu lub spadku) wartości badanej zmiennej. Jest rozpatrywana jako konsekwencja działania stałego zestawu czynników.\n",
        "* Wahania cykliczne (składowa cykliczna) wyrażają się w postaci długookresowych, rytmicznych wahań wartości szeregu wokół tendencji rozwojowej. Wiąże się je zwykle z cyklem koniunkturalnym gospodarki.\n",
        "* Wahania sezonowe (składowa sezonowa) są wahaniami wartości szeregu wokół jego tendencji rozwojowej o okresie nie przekraczającym jednego roku. Reprezentują efekty powtarzające się z pewną prawidłowością, co roku w tych samych okresach. Odzwierciedlają zwykle wpływ pogody (związany głównie z następstwem pór roku) lub kalendarza.\n",
        "* Część resztowa, tj. nie podlegająca objaśnieniu (nie dająca się przypisać do wymienionych źródeł zmienności) nazywana jest składową przypadkową (niesystematyczną). Zawiera ona przypadkowe wahania szeregu wokół części systematycznej, które trudno jest zidentyfikować a priori.\n",
        "\n",
        "źródło: https://stat.gov.pl/metainformacje/szeregi-czasowe-4712/\n",
        "\n",
        "\n",
        "Przykłady: autoregresja, ARIMA, SARIMA, SARIMAX, blibloteka Prophet, rekurencyjne sieci neuronowe."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "609bfb69",
      "metadata": {
        "id": "609bfb69"
      },
      "source": [
        "Przykładowe gotowe modele w python: \n",
        "    \n",
        "* https://machinelearningmastery.com/arima-for-time-series-forecasting-with-python/\n",
        "* https://towardsdatascience.com/prophet-in-a-loop-a875516ef2f9\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0da6f69",
      "metadata": {
        "id": "f0da6f69"
      },
      "source": [
        "#### Wyszukiwanie społeczności w grafach:\n",
        "\n",
        "Dzielenie grafów (sieci) na części. Przykład: graf w którym wierzchołek jest osobą, a krawędź połączeniem telefonicznym wykonanym do innej osoby (wierzchołka), wagą (siłą połączenia) może być tu np. długość rozmowy. W takim grafie można wybranymi algorytmami wyszukać społeczności.\n",
        "\n",
        "Przykład: https://www.geeksforgeeks.org/networkx-python-software-package-study-complex-networks/\n",
        "\n",
        "Biblioteka: networkx"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f58f8ed",
      "metadata": {
        "id": "1f58f8ed"
      },
      "source": [
        "#### Optymalizacja dyskretna\n",
        "\n",
        "Przykład: Problem komiwojażera przedstawiciel handlowy musi odwiedzić pewną liczbę klientów w różnych miastach i wrócić do początkowego punktu. Jak wybrać optymalną trasę?"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}