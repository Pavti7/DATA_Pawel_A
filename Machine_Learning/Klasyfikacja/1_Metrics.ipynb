{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-g4zNhPSX4BW"
   },
   "source": [
    "# Metryki klasyfikacji\n",
    "\n",
    "Aby poznać popularne metody oceny klasyfikatorów wygenerujemy sobie przykładowe sekwencje klas przykładów. Wykorzystamy w tym celu bibliotekę numpy oraz generator liczb losowych z rozkładu normalnego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import RandomState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random = RandomState(30)\n",
    "\n",
    "random_1 = random.normal(loc = 0.0, size = 100)\n",
    "random_2 = random.logistic(loc = 1, size = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.26405266  1.52790535 -0.97071094  0.47055962 -0.10069672  0.30379318\n",
      " -1.72596243  1.58509537  0.13429659 -1.10685547]\n",
      "[-0.34625296  2.7465497   0.46063247  3.2000744   0.12353606  1.56724961\n",
      "  2.32137633 -0.21323077  1.9572528   0.03456367]\n"
     ]
    }
   ],
   "source": [
    "print(random_1[:10])\n",
    "print(random_2[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = [1 if i >=0 else 0 for i in random_1]\n",
    "y_pred = [1 if i >=0 else 0 for i in random_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 0, 1, 0, 1, 0, 1, 1, 0]\n",
      "[0, 1, 1, 1, 1, 1, 1, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_test[:10])# test wynik właściwy\n",
    "print(y_pred[:10])# porównanie "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbK2vwiXX4Be"
   },
   "source": [
    "## Macierz błędów\n",
    "\n",
    "Pierwszym krokiem będzie stworzenie macierzy błędów, czyli wyznaczenia true postives, true negatives, false positives i false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(truth, prediction):\n",
    "    tp, tn, fp, fn = 0, 0, 0, 0\n",
    "    \n",
    "    for label_t, label_p in zip(truth, prediction):# finkcja \"zip\" porównuje zmienne 1 do 1 na zasadzie zamka błyskawicznego\n",
    "        if label_t == label_p:\n",
    "            if label_p == 1:\n",
    "                tp +=1\n",
    "            else:\n",
    "                tn +=1\n",
    "        else:\n",
    "            if label_p == 1:\n",
    "                fp +=1\n",
    "            else:\n",
    "                fn +=1\n",
    "    return tp, tn, fp, fn\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp, tn, fp, fn = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP:30, TN:14\n",
      "Ile obserwacji zaklasyfikowaliśmy poprawnie: 44\n"
     ]
    }
   ],
   "source": [
    "print(f'TP:{tp}, TN:{tn}')\n",
    "print(f'Ile obserwacji zaklasyfikowaliśmy poprawnie: {tp+tn}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP:30, TN:13\n",
      "Ile obserwacji zaklasyfikowaliśmy poprawnie: 56\n"
     ]
    }
   ],
   "source": [
    "print(f'FP:{tp}, TN:{fn}')\n",
    "print(f'Ile obserwacji zaklasyfikowaliśmy poprawnie: {fp+fn}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wUCbfBRoX4Bf"
   },
   "source": [
    "## Accuracy\n",
    "\n",
    "Pierwszą miarą jest miara dokładności "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accuracy = (tp+tn) / (tp+tn+fp+fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(truth, prediction):\n",
    "    tp, tn, fp, fn = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    return (tp+tn) / (tp+tn+fp+fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uzN5wkknX4Bf"
   },
   "source": [
    "Wynik dla analizowanych predykcji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dokładność: 0.44\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy(y_test, y_pred)\n",
    "print(f'Dokładność: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EDgmQCqzX4Bg"
   },
   "source": [
    "#### Problem\n",
    "Accuracy zachowuje się źle przy niezbalansowanych zbiorach. Wygenerujemy zbiór, w którym większość przykładów będzie negatywna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_3 = random.normal(loc = -1.5, size = 100)\n",
    "y_pred_2 = [1 if i >= 0 else 0 for i in random_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_pred_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43maccuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred_2\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'float' object is not callable"
     ]
    }
   ],
   "source": [
    "accuracy(y_test, y_pred_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFio7HseX4Bh"
   },
   "source": [
    "Zakładając z góry, że wszystkie przykłady są negatywne uzyskujemy bardzo wysoką dokładność. Takie zachowanie jest w wielu przypadkach niepożądane - przypuśćmy, że próbujemy stworzyć klasyfikator do komórek rakowych, gdzie większość przypadków jest negatywna - dla takiego podejścia, zwrócenie informacji że wszystkie przypadki są negatywne da bardzo wysoką dokładność."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 0, 1, 0, 1, 0, 1, 1, 0]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 1, 1, 1, 1, 1, 0, 1, 1]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 0, 0, 0, 0, 1, 0, 1, 0]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_2[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Y6fcinkX4Bi"
   },
   "source": [
    "## Recall\n",
    "\n",
    "Ta miara z kolei mówi o tym, jaką część dodatnich wyników wykrył klasyfikator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "recall = tp / (tp + fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(truth, prediction):\n",
    "    tp, tn, fp, fn = confusion_metrix(truth, prediction)\n",
    "    \n",
    "    return tp/(tp+tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'confusion_metrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mrecall\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[21], line 2\u001b[0m, in \u001b[0;36mrecall\u001b[1;34m(truth, prediction)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrecall\u001b[39m(truth, prediction):\n\u001b[1;32m----> 2\u001b[0m     tp, tn, fp, fn \u001b[38;5;241m=\u001b[39m \u001b[43mconfusion_metrix\u001b[49m(truth, prediction)\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tp\u001b[38;5;241m/\u001b[39m(tp\u001b[38;5;241m+\u001b[39mtn)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'confusion_metrix' is not defined"
     ]
    }
   ],
   "source": [
    "recall(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'confusion_metrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mrecall\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred_2\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[21], line 2\u001b[0m, in \u001b[0;36mrecall\u001b[1;34m(truth, prediction)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrecall\u001b[39m(truth, prediction):\n\u001b[1;32m----> 2\u001b[0m     tp, tn, fp, fn \u001b[38;5;241m=\u001b[39m \u001b[43mconfusion_metrix\u001b[49m(truth, prediction)\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tp\u001b[38;5;241m/\u001b[39m(tp\u001b[38;5;241m+\u001b[39mtn)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'confusion_metrix' is not defined"
     ]
    }
   ],
   "source": [
    "recall(y_test, y_pred_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcIZ3w-LX4Bj"
   },
   "source": [
    "## Precision\n",
    "\n",
    "Jest to miara, która skupia się tylko na przykładach pozytywnych - mówi jaka część wyników wskazanych przez klasyfikator jako dodatnie jest rzeczywiście dodatnia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "precision = tp / (tp+fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(truth, prediction):\n",
    "    tp, tn, fp, fn = confusion_metrix(truth, prediction)\n",
    "    \n",
    "    return tp/(tp+tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction(y_test, y_pred_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g9SoQbRPX4Bk"
   },
   "source": [
    "# F1 Score\n",
    "\n",
    "Jest to średnia harmoniczna precyzji i czułości. Ogólnie - im wyższy F1-score tym lepszy jest klasyfikator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f_score = (2 * precision * recall)/(prec+rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(truth, predicted):\n",
    "    prec=precision(truth, predicted)\n",
    "    rec=recall(truth, predicted)\n",
    "    \n",
    "    return 2*prec*rec / (prec+rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test, y_pred_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AnZA6fKdX4Bm"
   },
   "source": [
    "**Zadanie:** Sprawdź funckje f1_score, precision_score, recall_score z modułu scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m f1_score, precision_score, recall_score, confusion_matrix, classification_report\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Metrics.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
